{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3d727-7222-4774-8256-7550d65ebc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as WordDocument\n",
    "\n",
    "CHROMA_PATH = \"chroma\"  # Path to Chroma database\n",
    "DATA_PATH = \"SOPS\"  # Path to SOPS directory\n",
    "MANUAL_FILE = \"summit-oa-user-guide-en.pdf\"  # Full path to the manual\n",
    "OUTPUT_FILE = \"Generated_SOP_for_Gas_Chromatography.docx\"  # Output path for SOP document\n",
    "API_KEY = \"private key\"\n",
    "# Step 1: Load documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create or update Chroma database\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding_function():\n",
    "    return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Step 4: Generate a single SOP section\n",
    "def generate_sop_section(query_text: str, manual_path: str, section: str):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # Extract manual content\n",
    "    manual_text = extract_manual_content(manual_path, max_pages=10)\n",
    "    context_text += f\"\\n\\nManual Content:\\n{manual_text[:2000]}\"  # Include snippet of manual content\n",
    "\n",
    "    # Generate the section\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to create the '{section}' section for the SOP:\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Section: {section}\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content if response else \"Error generating this section\"\n",
    "\n",
    "# Step 5: Extract content from PDF (without images)\n",
    "def extract_manual_content(pdf_path, max_pages=10):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(min(len(doc), max_pages)):\n",
    "        page = doc[page_num]\n",
    "        text.append(page.get_text())\n",
    "\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Step 6: Save SOP to Word (without images)\n",
    "def save_sop_to_word(sop_text: str, output_path: str):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    doc = WordDocument()\n",
    "    doc.add_heading(\"Standard Operating Procedure (SOP)\", level=1)\n",
    "\n",
    "    # Split SOP text into sections\n",
    "    sections = sop_text.split(\"\\n\\n\")\n",
    "    for section in sections:\n",
    "        if \":\" in section:\n",
    "            header, content = section.split(\":\", 1)\n",
    "            doc.add_heading(header.strip(), level=2)\n",
    "            doc.add_paragraph(content.strip())\n",
    "        else:\n",
    "            doc.add_paragraph(section.strip())\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"SOP successfully saved to {output_path}\")\n",
    "\n",
    "# Step 7: Generate the full SOP\n",
    "def generate_full_sop(query_text: str, manual_path: str, output_path: str):\n",
    "    sop_text = \"\"\n",
    "    sop_sections = [\n",
    "        \"Scope\",\n",
    "        \"Background\",\n",
    "        \"Safety\",\n",
    "        \"Materials Required\",\n",
    "        \"Standards and Controls\",\n",
    "        \"Calibration\",\n",
    "        \"Procedures\",\n",
    "        \"Sampling\",\n",
    "        \"Calculations\",\n",
    "        \"Uncertainty of Measurement\",\n",
    "        \"Limitations\",\n",
    "        \"Documentation\",\n",
    "        \"References\",\n",
    "    ]\n",
    "\n",
    "    for section in sop_sections:\n",
    "        print(f\"Generating section: {section}\")\n",
    "        section_text = generate_sop_section(query_text, manual_path, section)\n",
    "        sop_text += f\"\\n\\n{section}:\\n{section_text}\"\n",
    "\n",
    "    save_sop_to_word(sop_text, output_path)\n",
    "    print(\"SOP generation completed.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    print(\"Step 1: Loading documents...\")\n",
    "    documents = load_documents()\n",
    "\n",
    "    print(\"Step 2: Splitting documents into chunks...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    print(\"Step 3: Adding chunks to Chroma database...\")\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    print(\"Step 4: Generating SOP for an instrument...\")\n",
    "    instrument_query = \"Generate SOP for FTIR Spectrometer for the brand thermo Fisher for polymer\"\n",
    "    generate_full_sop(instrument_query, MANUAL_FILE, OUTPUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc5aef1-3a1c-46a3-a443-0bd88e2d7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOP 'GCMS-6.pdf' not found in 'SOPS'.\n",
      "Step 1: Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Splitting documents into chunks...\n",
      "Step 3: Adding chunks to Chroma database...\n",
      "Step 4: Generating SOP for an instrument...\n",
      "Generating section: Scope\n",
      "Generating section: Background\n",
      "Generating section: Safety\n",
      "Generating section: Materials Required\n",
      "Generating section: Standards and Controls\n",
      "Generating section: Calibration\n",
      "Generating section: Procedures\n",
      "Generating section: Sampling\n",
      "Generating section: Calculations\n",
      "Generating section: Uncertainty of Measurement\n",
      "Generating section: Limitations\n",
      "Generating section: Documentation\n",
      "Generating section: References\n",
      "SOP successfully saved to Generated_SOP_for_Gas_Chromatography.docx\n",
      "SOP generation completed.\n",
      "Similarity score between generated and actual SOP: 0.80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as WordDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "CHROMA_PATH = \"chroma\"  # Path to Chroma database\n",
    "DATA_PATH = \"SOPS\"  # Path to SOPS directory\n",
    "HIDDEN_SOP = \"GCMS-6.pdf\"  # Name of the hidden SOP file\n",
    "MANUAL_FILE = \"c10g-e080.pdf\"  # Manual file\n",
    "OUTPUT_FILE = \"Generated_SOP_for_Gas_Chromatography.docx\"  # Output file\n",
    "API_KEY = \"private key\"\n",
    "\n",
    "# Step 1: Load documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create or update Chroma database\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding_function():\n",
    "    return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Step 4: Generate a single SOP section\n",
    "def generate_sop_section(query_text: str, manual_file: str, section: str):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # Extract manual content\n",
    "    manual_text = extract_manual_content(manual_file, max_pages=10)\n",
    "    context_text += f\"\\n\\nManual Content:\\n{manual_text[:2000]}\"  # Include snippet of manual content\n",
    "\n",
    "    # Generate the section\n",
    "    prompt = f\"\"\"\n",
    "     # Updated prompt\n",
    "    prompt = f\"\"\n",
    "    Use the following context to create a detailed Standard Operating Procedure (SOP). \n",
    "    Include comprehensive details for each section, including examples, explanations, and visuals. Don't summarize the details and keep the ideas unique and not redundant. \n",
    "        Ensure the content follows these guidelines:\n",
    "        - Use formal and technical language typical of SOPs.\n",
    "        - Align the structure and terminology with the provided context.\n",
    "        - Maintain clarity and conciseness.\n",
    "        Context:\n",
    "    Structure:\n",
    "    1. Scope: Defines the purpose and applicability of the procedure, often focusing on specific uses, such as analyzing air samples, tobacco content, or hydrocarbons in automotive exhaust.\n",
    "    2. Background: Provides context or rationale for the method being standardized, often referencing related research or industry needs.\n",
    "    3. Safety: Highlights safety precautions, including the use of personal protective equipment (PPE), handling hazardous chemicals, and waste disposal protocols.\n",
    "    4. Materials and Equipment: Lists all necessary reagents, solvents, instrumentation, and ancillary tools required for conducting the procedure.\n",
    "    5. Standards and Calibration: Details the required reference materials, standards for quality control, and steps for instrument calibration to ensure accuracy.\n",
    "    6. Procedure: Offers step-by-step instructions for conducting the analysis, including sample preparation, injection, instrument operation, and troubleshooting.\n",
    "    7. Quality Control and Data Validation: Focuses on performance checks, validation criteria, and measures to ensure data reliability.\n",
    "    8. Data Analysis and Reporting: Describes how results should be processed, analyzed, and documented, often including specific formats or tools.\n",
    "    9. Limitations and Interferences: Identifies constraints of the method, such as compounds that may not be detected or interferences that could affect results.\n",
    "    10. Health and Environmental Safety: Emphasizes precautions to protect operators and minimize environmental impact during the procedure.\n",
    "    11. Documentation and References: Specifies record-keeping requirements and cites foundational literature or external standards that the SOP builds upon.\n",
    "    12. Special Precautions and Handling: Covers specific handling instructions for sens\n",
    "    Use the following context to create the '{section}' section for the SOP:\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Section: {section}\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content if response else \"Error generating this section\"\n",
    "\n",
    "# Step 5: Extract content from PDF (without images)\n",
    "def extract_manual_content(pdf_path, max_pages=10):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(min(len(doc), max_pages)):\n",
    "        page = doc[page_num]\n",
    "        text.append(page.get_text())\n",
    "\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Step 6: Save SOP to Word (without images)\n",
    "def save_sop_to_word(sop_text: str, output_file: str):\n",
    "    doc = WordDocument()\n",
    "    doc.add_heading(\"Standard Operating Procedure (SOP)\", level=1)\n",
    "\n",
    "    # Split SOP text into sections\n",
    "    sections = sop_text.split(\"\\n\\n\")\n",
    "    for section in sections:\n",
    "        if \":\" in section:\n",
    "            header, content = section.split(\":\", 1)\n",
    "            doc.add_heading(header.strip(), level=2)\n",
    "            doc.add_paragraph(content.strip())\n",
    "        else:\n",
    "            doc.add_paragraph(section.strip())\n",
    "\n",
    "    doc.save(output_file)\n",
    "    print(f\"SOP successfully saved to {output_file}\")\n",
    "\n",
    "# Step 7: Generate the full SOP\n",
    "def generate_full_sop(query_text: str, manual_file: str, output_file: str):\n",
    "    sop_text = \"\"\n",
    "    sop_sections = [\n",
    "        \"Scope\",\n",
    "        \"Background\",\n",
    "        \"Safety\",\n",
    "        \"Materials Required\",\n",
    "        \"Standards and Controls\",\n",
    "        \"Calibration\",\n",
    "        \"Procedures\",\n",
    "        \"Sampling\",\n",
    "        \"Calculations\",\n",
    "        \"Uncertainty of Measurement\",\n",
    "        \"Limitations\",\n",
    "        \"Documentation\",\n",
    "        \"References\",\n",
    "    ]\n",
    "\n",
    "    for section in sop_sections:\n",
    "        print(f\"Generating section: {section}\")\n",
    "        section_text = generate_sop_section(query_text, manual_file, section)\n",
    "        sop_text += f\"\\n\\n{section}:\\n{section_text}\"\n",
    "\n",
    "    save_sop_to_word(sop_text, output_file)\n",
    "    print(\"SOP generation completed.\")\n",
    "\n",
    "# Step 8: Hide one SOP\n",
    "def hide_sop(sop_name: str, source_dir: str, hidden_dir: str):\n",
    "    sop_path = os.path.join(source_dir, sop_name)\n",
    "    hidden_path = os.path.join(hidden_dir, sop_name)\n",
    "\n",
    "    if not os.path.exists(hidden_dir):\n",
    "        os.makedirs(hidden_dir)\n",
    "\n",
    "    if os.path.exists(sop_path):\n",
    "        try:\n",
    "            shutil.copy(sop_path, hidden_path)\n",
    "            os.remove(sop_path)\n",
    "            print(f\"SOP '{sop_name}' has been hidden.\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"Permission error: {e}. Ensure the file is not open in another program.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while hiding SOP: {e}\")\n",
    "    else:\n",
    "        print(f\"SOP '{sop_name}' not found in '{source_dir}'.\")\n",
    "\n",
    "# Step 9: Extract and Compare File Content\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = WordDocument(docx_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def calculate_similarity(file1_path: str, file2_path: str):\n",
    "    if file1_path.endswith('.docx'):\n",
    "        file1_content = extract_text_from_docx(file1_path)\n",
    "    elif file1_path.endswith('.pdf'):\n",
    "        file1_content = extract_text_from_pdf(file1_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file1_path}\")\n",
    "\n",
    "    if file2_path.endswith('.docx'):\n",
    "        file2_content = extract_text_from_docx(file2_path)\n",
    "    elif file2_path.endswith('.pdf'):\n",
    "        file2_content = extract_text_from_pdf(file2_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file2_path}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit_transform([file1_content, file2_content])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0, 1]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    hide_sop(HIDDEN_SOP, DATA_PATH, \"hidden_sops\")\n",
    "\n",
    "    print(\"Step 1: Loading documents...\")\n",
    "    documents = load_documents()\n",
    "\n",
    "    print(\"Step 2: Splitting documents into chunks...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    print(\"Step 3: Adding chunks to Chroma database...\")\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    print(\"Step 4: Generating SOP for an instrument...\")\n",
    "    instrument_query = \"Write the SOP for Shimadzu Gas chromatography mass spectrometer QP2010 SE Standard Operating Procedure\"\n",
    "    generate_full_sop(instrument_query, MANUAL_FILE, OUTPUT_FILE)\n",
    "\n",
    "    hidden_sop_path = os.path.join(\"hidden_sops\", HIDDEN_SOP)\n",
    "    similarity_score = calculate_similarity(OUTPUT_FILE, hidden_sop_path)\n",
    "    print(f\"Similarity score between generated and actual SOP: {similarity_score:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e11be8-8a1e-4be5-9008-abbc6959c3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOP 'GCMS-6.pdf' not found in 'SOPS'.\n",
      "Step 1: Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Splitting documents into chunks...\n",
      "Step 3: Adding chunks to Chroma database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roxan\\AppData\\Local\\Temp\\ipykernel_26024\\3821707587.py:39: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
      "C:\\Users\\roxan\\AppData\\Local\\Temp\\ipykernel_26024\\3821707587.py:33: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
      "C:\\Users\\roxan\\AppData\\Local\\Temp\\ipykernel_26024\\3821707587.py:35: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Generating SOP for an instrument...\n",
      "Generating section: Scope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roxan\\AppData\\Local\\Temp\\ipykernel_26024\\3821707587.py:83: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating section: Background\n",
      "Generating section: Safety\n",
      "Generating section: Materials Required\n",
      "Generating section: Standards and Controls\n",
      "Generating section: Calibration\n",
      "Generating section: Procedures\n",
      "Generating section: Sampling\n",
      "Generating section: Calculations\n",
      "Generating section: Uncertainty of Measurement\n",
      "Generating section: Limitations\n",
      "Generating section: Documentation\n",
      "Generating section: References\n",
      "SOP successfully saved to Generated_SOP_for_Gas_Chromatography.docx\n",
      "SOP generation completed.\n",
      "Similarity score between generated and actual SOP: 0.83\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as WordDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "CHROMA_PATH = \"chroma\"  # Path to Chroma database\n",
    "DATA_PATH = \"SOPS\"  # Path to SOPS directory\n",
    "HIDDEN_SOP = \"GCMS-6.pdf\"  # Name of the hidden SOP file\n",
    "MANUAL_FILE = \"c10g-e080.pdf\"  # Manual file\n",
    "OUTPUT_FILE = \"Generated_SOP_for_Gas_Chromatography.docx\"  # Output file\n",
    "API_KEY = \"private key\"\n",
    "# Step 1: Load documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create or update Chroma database\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding_function():\n",
    "    return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Step 4: Generate a single SOP section\n",
    "def generate_sop_section(query_text: str, manual_file: str, section: str):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # Extract manual content\n",
    "    manual_text = extract_manual_content(manual_file, max_pages=10)\n",
    "    context_text += f\"\\n\\nManual Content:\\n{manual_text[:2000]}\"  # Include snippet of manual content\n",
    "\n",
    "    # Generate the section\n",
    "    prompt = f\"\"\"\n",
    "     # Updated prompt\n",
    "    prompt = f\"\"\n",
    "    Use the following context to create a detailed Standard Operating Procedure (SOP). \n",
    "    Include comprehensive details for each section, including examples, explanations, and visuals. Don't summarize the details and keep the ideas unique and not redundant. \n",
    "        Ensure the content follows these guidelines:\n",
    "        - Use formal and technical language typical of SOPs.\n",
    "        - Align the structure and terminology with the provided context.\n",
    "        - Maintain clarity and conciseness.\n",
    "        Context:\n",
    "    Structure:\n",
    "    1. Scope: Defines the purpose and applicability of the procedure, often focusing on specific uses, such as analyzing air samples, tobacco content, or hydrocarbons in automotive exhaust.\n",
    "    2. Background: Provides context or rationale for the method being standardized, often referencing related research or industry needs.\n",
    "    3. Safety: Highlights safety precautions, including the use of personal protective equipment (PPE), handling hazardous chemicals, and waste disposal protocols.\n",
    "    4. Materials and Equipment: Lists all necessary reagents, solvents, instrumentation, and ancillary tools required for conducting the procedure.\n",
    "    5. Standards and Calibration: Details the required reference materials, standards for quality control, and steps for instrument calibration to ensure accuracy.\n",
    "    6. Procedure: Offers step-by-step instructions for conducting the analysis, including sample preparation, injection, instrument operation, and troubleshooting.\n",
    "    7. Quality Control and Data Validation: Focuses on performance checks, validation criteria, and measures to ensure data reliability.\n",
    "    8. Data Analysis and Reporting: Describes how results should be processed, analyzed, and documented, often including specific formats or tools.\n",
    "    9. Limitations and Interferences: Identifies constraints of the method, such as compounds that may not be detected or interferences that could affect results.\n",
    "    10. Health and Environmental Safety: Emphasizes precautions to protect operators and minimize environmental impact during the procedure.\n",
    "    11. Documentation and References: Specifies record-keeping requirements and cites foundational literature or external standards that the SOP builds upon.\n",
    "    12. Special Precautions and Handling: Covers specific handling instructions for sens\n",
    "    Use the following context to create the '{section}' section for the SOP:\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Section: {section}\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content if response else \"Error generating this section\"\n",
    "\n",
    "# Step 5: Extract content from PDF (without images)\n",
    "def extract_manual_content(pdf_path, max_pages=10):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(min(len(doc), max_pages)):\n",
    "        page = doc[page_num]\n",
    "        text.append(page.get_text())\n",
    "\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Step 6: Save SOP to Word (without images)\n",
    "def save_sop_to_word(sop_text: str, output_file: str):\n",
    "    doc = WordDocument()\n",
    "    doc.add_heading(\"Standard Operating Procedure (SOP)\", level=1)\n",
    "\n",
    "    # Split SOP text into sections\n",
    "    sections = sop_text.split(\"\\n\\n\")\n",
    "    for section in sections:\n",
    "        if \":\" in section:\n",
    "            header, content = section.split(\":\", 1)\n",
    "            doc.add_heading(header.strip(), level=2)\n",
    "            doc.add_paragraph(content.strip())\n",
    "        else:\n",
    "            doc.add_paragraph(section.strip())\n",
    "\n",
    "    doc.save(output_file)\n",
    "    print(f\"SOP successfully saved to {output_file}\")\n",
    "\n",
    "# Step 7: Generate the full SOP\n",
    "def generate_full_sop(query_text: str, manual_file: str, output_file: str):\n",
    "    sop_text = \"\"\n",
    "    sop_sections = [\n",
    "        \"Scope\",\n",
    "        \"Background\",\n",
    "        \"Safety\",\n",
    "        \"Materials Required\",\n",
    "        \"Standards and Controls\",\n",
    "        \"Calibration\",\n",
    "        \"Procedures\",\n",
    "        \"Sampling\",\n",
    "        \"Calculations\",\n",
    "        \"Uncertainty of Measurement\",\n",
    "        \"Limitations\",\n",
    "        \"Documentation\",\n",
    "        \"References\",\n",
    "    ]\n",
    "\n",
    "    for section in sop_sections:\n",
    "        print(f\"Generating section: {section}\")\n",
    "        section_text = generate_sop_section(query_text, manual_file, section)\n",
    "        sop_text += f\"\\n\\n{section}:\\n{section_text}\"\n",
    "\n",
    "    save_sop_to_word(sop_text, output_file)\n",
    "    print(\"SOP generation completed.\")\n",
    "\n",
    "# Step 8: Hide one SOP\n",
    "def hide_sop(sop_name: str, source_dir: str, hidden_dir: str):\n",
    "    sop_path = os.path.join(source_dir, sop_name)\n",
    "    hidden_path = os.path.join(hidden_dir, sop_name)\n",
    "\n",
    "    if not os.path.exists(hidden_dir):\n",
    "        os.makedirs(hidden_dir)\n",
    "\n",
    "    if os.path.exists(sop_path):\n",
    "        try:\n",
    "            shutil.copy(sop_path, hidden_path)\n",
    "            os.remove(sop_path)\n",
    "            print(f\"SOP '{sop_name}' has been hidden.\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"Permission error: {e}. Ensure the file is not open in another program.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while hiding SOP: {e}\")\n",
    "    else:\n",
    "        print(f\"SOP '{sop_name}' not found in '{source_dir}'.\")\n",
    "\n",
    "# Step 9: Extract and Compare File Content\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = WordDocument(docx_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def calculate_similarity(file1_path: str, file2_path: str):\n",
    "    if file1_path.endswith('.docx'):\n",
    "        file1_content = extract_text_from_docx(file1_path)\n",
    "    elif file1_path.endswith('.pdf'):\n",
    "        file1_content = extract_text_from_pdf(file1_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file1_path}\")\n",
    "\n",
    "    if file2_path.endswith('.docx'):\n",
    "        file2_content = extract_text_from_docx(file2_path)\n",
    "    elif file2_path.endswith('.pdf'):\n",
    "        file2_content = extract_text_from_pdf(file2_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file2_path}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit_transform([file1_content, file2_content])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0, 1]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    hide_sop(HIDDEN_SOP, DATA_PATH, \"hidden_sops\")\n",
    "\n",
    "    print(\"Step 1: Loading documents...\")\n",
    "    documents = load_documents()\n",
    "\n",
    "    print(\"Step 2: Splitting documents into chunks...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    print(\"Step 3: Adding chunks to Chroma database...\")\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    print(\"Step 4: Generating SOP for an instrument...\")\n",
    "    instrument_query = \"Write the SOP for Shimadzu Gas chromatography mass spectrometer QP2010 SE Standard Operating Procedure\"\n",
    "    generate_full_sop(instrument_query, MANUAL_FILE, OUTPUT_FILE)\n",
    "\n",
    "    hidden_sop_path = os.path.join(\"hidden_sops\", HIDDEN_SOP)\n",
    "    similarity_score = calculate_similarity(OUTPUT_FILE, hidden_sop_path)\n",
    "    print(f\"Similarity score between generated and actual SOP: {similarity_score:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c7c1ea-9bc5-436d-ad92-01361164bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOP 'SOP_FTIR.pdf' has been hidden.\n",
      "Step 1: Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Splitting documents into chunks...\n",
      "Step 3: Adding chunks to Chroma database...\n",
      "Step 4: Generating SOP for an instrument...\n",
      "Generating section: Scope\n",
      "Generating section: Background\n",
      "Generating section: Safety\n",
      "Generating section: Materials Required\n",
      "Generating section: Standards and Controls\n",
      "Generating section: Calibration\n",
      "Generating section: Procedures\n",
      "Generating section: Sampling\n",
      "Generating section: Calculations\n",
      "Generating section: Uncertainty of Measurement\n",
      "Generating section: Limitations\n",
      "Generating section: Documentation\n",
      "Generating section: References\n",
      "SOP successfully saved to Genrated_SOP_FTIR.docx\n",
      "SOP generation completed.\n",
      "Similarity score between generated and actual SOP: 0.79\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as WordDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "CHROMA_PATH = \"chroma\"  # Path to Chroma database\n",
    "DATA_PATH = \"SOPS\"  # Path to SOPS directory\n",
    "HIDDEN_SOP = \"SOP_FTIR.pdf\"  # Name of the hidden SOP file\n",
    "MANUAL_FILE = \"\"  # Manual file\n",
    "OUTPUT_FILE = \"Genrated_SOP_FTIR.docx\"  # Output file\n",
    "API_KEY = \"private key\"\n",
    "\n",
    "# Step 1: Load documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create or update Chroma database\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding_function():\n",
    "    return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Step 4: Generate a single SOP section\n",
    "def generate_sop_section(query_text: str, manual_file: str, section: str):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # Extract manual content\n",
    "    manual_text = extract_manual_content(manual_file, max_pages=10)\n",
    "    context_text += f\"\\n\\nManual Content:\\n{manual_text[:2000]}\"  # Include snippet of manual content\n",
    "\n",
    "    # Generate the section\n",
    "    prompt = f\"\"\"\n",
    "     # Updated prompt\n",
    "    prompt = f\"\"\n",
    "    Use the following context to create a detailed Standard Operating Procedure (SOP). \n",
    "    Include comprehensive details for each section, including examples, explanations, and visuals. Don't summarize the details and keep the ideas unique and not redundant. \n",
    "        Ensure the content follows these guidelines:\n",
    "        - Use formal and technical language typical of SOPs.\n",
    "        - Align the structure and terminology with the provided context.\n",
    "        - Maintain clarity and conciseness.\n",
    "        Context:\n",
    "    Structure:\n",
    "    1. Scope: Defines the purpose and applicability of the procedure, often focusing on specific uses, such as analyzing air samples, tobacco content, or hydrocarbons in automotive exhaust.\n",
    "    2. Background: Provides context or rationale for the method being standardized, often referencing related research or industry needs.\n",
    "    3. Safety: Highlights safety precautions, including the use of personal protective equipment (PPE), handling hazardous chemicals, and waste disposal protocols.\n",
    "    4. Materials and Equipment: Lists all necessary reagents, solvents, instrumentation, and ancillary tools required for conducting the procedure.\n",
    "    5. Standards and Calibration: Details the required reference materials, standards for quality control, and steps for instrument calibration to ensure accuracy.\n",
    "    6. Procedure: Offers step-by-step instructions for conducting the analysis, including sample preparation, injection, instrument operation, and troubleshooting.\n",
    "    7. Quality Control and Data Validation: Focuses on performance checks, validation criteria, and measures to ensure data reliability.\n",
    "    8. Data Analysis and Reporting: Describes how results should be processed, analyzed, and documented, often including specific formats or tools.\n",
    "    9. Limitations and Interferences: Identifies constraints of the method, such as compounds that may not be detected or interferences that could affect results.\n",
    "    10. Health and Environmental Safety: Emphasizes precautions to protect operators and minimize environmental impact during the procedure.\n",
    "    11. Documentation and References: Specifies record-keeping requirements and cites foundational literature or external standards that the SOP builds upon.\n",
    "    12. Special Precautions and Handling: Covers specific handling instructions for sens\n",
    "    Use the following context to create the '{section}' section for the SOP:\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Section: {section}\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content if response else \"Error generating this section\"\n",
    "\n",
    "# Step 5: Extract content from PDF (without images)\n",
    "def extract_manual_content(pdf_path, max_pages=10):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(min(len(doc), max_pages)):\n",
    "        page = doc[page_num]\n",
    "        text.append(page.get_text())\n",
    "\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Step 6: Save SOP to Word (without images)\n",
    "def save_sop_to_word(sop_text: str, output_file: str):\n",
    "    doc = WordDocument()\n",
    "    doc.add_heading(\"Standard Operating Procedure (SOP)\", level=1)\n",
    "\n",
    "    # Split SOP text into sections\n",
    "    sections = sop_text.split(\"\\n\\n\")\n",
    "    for section in sections:\n",
    "        if \":\" in section:\n",
    "            header, content = section.split(\":\", 1)\n",
    "            doc.add_heading(header.strip(), level=2)\n",
    "            doc.add_paragraph(content.strip())\n",
    "        else:\n",
    "            doc.add_paragraph(section.strip())\n",
    "\n",
    "    doc.save(output_file)\n",
    "    print(f\"SOP successfully saved to {output_file}\")\n",
    "\n",
    "# Step 7: Generate the full SOP\n",
    "def generate_full_sop(query_text: str, manual_file: str, output_file: str):\n",
    "    sop_text = \"\"\n",
    "    sop_sections = [\n",
    "        \"Scope\",\n",
    "        \"Background\",\n",
    "        \"Safety\",\n",
    "        \"Materials Required\",\n",
    "        \"Standards and Controls\",\n",
    "        \"Calibration\",\n",
    "        \"Procedures\",\n",
    "        \"Sampling\",\n",
    "        \"Calculations\",\n",
    "        \"Uncertainty of Measurement\",\n",
    "        \"Limitations\",\n",
    "        \"Documentation\",\n",
    "        \"References\",\n",
    "    ]\n",
    "\n",
    "    for section in sop_sections:\n",
    "        print(f\"Generating section: {section}\")\n",
    "        section_text = generate_sop_section(query_text, manual_file, section)\n",
    "        sop_text += f\"\\n\\n{section}:\\n{section_text}\"\n",
    "\n",
    "    save_sop_to_word(sop_text, output_file)\n",
    "    print(\"SOP generation completed.\")\n",
    "\n",
    "# Step 8: Hide one SOP\n",
    "def hide_sop(sop_name: str, source_dir: str, hidden_dir: str):\n",
    "    sop_path = os.path.join(source_dir, sop_name)\n",
    "    hidden_path = os.path.join(hidden_dir, sop_name)\n",
    "\n",
    "    if not os.path.exists(hidden_dir):\n",
    "        os.makedirs(hidden_dir)\n",
    "\n",
    "    if os.path.exists(sop_path):\n",
    "        try:\n",
    "            shutil.copy(sop_path, hidden_path)\n",
    "            os.remove(sop_path)\n",
    "            print(f\"SOP '{sop_name}' has been hidden.\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"Permission error: {e}. Ensure the file is not open in another program.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while hiding SOP: {e}\")\n",
    "    else:\n",
    "        print(f\"SOP '{sop_name}' not found in '{source_dir}'.\")\n",
    "\n",
    "# Step 9: Extract and Compare File Content\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = WordDocument(docx_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def calculate_similarity(file1_path: str, file2_path: str):\n",
    "    if file1_path.endswith('.docx'):\n",
    "        file1_content = extract_text_from_docx(file1_path)\n",
    "    elif file1_path.endswith('.pdf'):\n",
    "        file1_content = extract_text_from_pdf(file1_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file1_path}\")\n",
    "\n",
    "    if file2_path.endswith('.docx'):\n",
    "        file2_content = extract_text_from_docx(file2_path)\n",
    "    elif file2_path.endswith('.pdf'):\n",
    "        file2_content = extract_text_from_pdf(file2_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file2_path}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit_transform([file1_content, file2_content])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0, 1]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    hide_sop(HIDDEN_SOP, DATA_PATH, \"hidden_sops\")\n",
    "\n",
    "    print(\"Step 1: Loading documents...\")\n",
    "    documents = load_documents()\n",
    "\n",
    "    print(\"Step 2: Splitting documents into chunks...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    print(\"Step 3: Adding chunks to Chroma database...\")\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    print(\"Step 4: Generating SOP for an instrument...\")\n",
    "    instrument_query = \"Write the SOP for the Perkin Elmer model 1600 FTIR.\"\n",
    "    generate_full_sop(instrument_query, MANUAL_FILE, OUTPUT_FILE)\n",
    "\n",
    "    hidden_sop_path = os.path.join(\"hidden_sops\", HIDDEN_SOP)\n",
    "    similarity_score = calculate_similarity(OUTPUT_FILE, hidden_sop_path)\n",
    "    print(f\"Similarity score between generated and actual SOP: {similarity_score:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c801c94-a15f-49bd-969e-7a6784cd4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOP 'ICPMS_SOPs_Liquids.pdf' not found in 'SOPS'.\n",
      "Step 1: Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Splitting documents into chunks...\n",
      "Step 3: Adding chunks to Chroma database...\n",
      "Step 4: Generating SOP for an instrument...\n",
      "Generating section: Scope\n",
      "Generating section: Background\n",
      "Generating section: Safety\n",
      "Generating section: Materials Required\n",
      "Generating section: Standards and Controls\n",
      "Generating section: Calibration\n",
      "Generating section: Procedures\n",
      "Generating section: Sampling\n",
      "Generating section: Calculations\n",
      "Generating section: Uncertainty of Measurement\n",
      "Generating section: Limitations\n",
      "Generating section: Documentation\n",
      "Generating section: References\n",
      "SOP successfully saved to Generated_SOP_for_ICP.docx\n",
      "SOP generation completed.\n",
      "Similarity score between generated and actual SOP: 0.82\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as WordDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "CHROMA_PATH = \"chroma\"  # Path to Chroma database\n",
    "DATA_PATH = \"SOPS\"  # Path to SOPS directory\n",
    "HIDDEN_SOP = \"ICPMS_SOPs_Liquids.pdf\"  # Name of the hidden SOP file\n",
    "MANUAL_FILE = \"8510230100_700SeriesICP_UserManual.pdf\"  # Manual file\n",
    "OUTPUT_FILE = \"Generated_SOP_for_ICP.docx\"  # Output file\n",
    "API_KEY = \"private key\"\n",
    "\n",
    "# Step 1: Load documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Step 2: Split documents into chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create or update Chroma database\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding_function():\n",
    "    return OpenAIEmbeddings(openai_api_key=API_KEY)\n",
    "\n",
    "# Step 4: Generate a single SOP section\n",
    "def generate_sop_section(query_text: str, manual_file: str, section: str):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # Extract manual content\n",
    "    manual_text = extract_manual_content(manual_file, max_pages=10)\n",
    "    context_text += f\"\\n\\nManual Content:\\n{manual_text[:2000]}\"  # Include snippet of manual content\n",
    "\n",
    "    # Generate the section\n",
    "    prompt = f\"\"\"\n",
    "     # Updated prompt\n",
    "    prompt = f\"\"\n",
    "    Use the following context to create a detailed Standard Operating Procedure (SOP). \n",
    "    Include comprehensive details for each section, including examples, explanations, and visuals. Don't summarize the details and keep the ideas unique and not redundant. \n",
    "        Ensure the content follows these guidelines:\n",
    "        - Use formal and technical language typical of SOPs.\n",
    "        - Align the structure and terminology with the provided context.\n",
    "        - Maintain clarity and conciseness.\n",
    "        Context:\n",
    "    Structure:\n",
    "    1. Scope: Defines the purpose and applicability of the procedure, often focusing on specific uses, such as analyzing air samples, tobacco content, or hydrocarbons in automotive exhaust.\n",
    "    2. Background: Provides context or rationale for the method being standardized, often referencing related research or industry needs.\n",
    "    3. Safety: Highlights safety precautions, including the use of personal protective equipment (PPE), handling hazardous chemicals, and waste disposal protocols.\n",
    "    4. Materials and Equipment: Lists all necessary reagents, solvents, instrumentation, and ancillary tools required for conducting the procedure.\n",
    "    5. Standards and Calibration: Details the required reference materials, standards for quality control, and steps for instrument calibration to ensure accuracy.\n",
    "    6. Procedure: Offers step-by-step instructions for conducting the analysis, including sample preparation, injection, instrument operation, and troubleshooting.\n",
    "    7. Quality Control and Data Validation: Focuses on performance checks, validation criteria, and measures to ensure data reliability.\n",
    "    8. Data Analysis and Reporting: Describes how results should be processed, analyzed, and documented, often including specific formats or tools.\n",
    "    9. Limitations and Interferences: Identifies constraints of the method, such as compounds that may not be detected or interferences that could affect results.\n",
    "    10. Health and Environmental Safety: Emphasizes precautions to protect operators and minimize environmental impact during the procedure.\n",
    "    11. Documentation and References: Specifies record-keeping requirements and cites foundational literature or external standards that the SOP builds upon.\n",
    "    12. Special Precautions and Handling: Covers specific handling instructions for sens\n",
    "    Use the following context to create the '{section}' section for the SOP:\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Section: {section}\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4\", openai_api_key=API_KEY)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return response.content if response else \"Error generating this section\"\n",
    "\n",
    "# Step 5: Extract content from PDF (without images)\n",
    "def extract_manual_content(pdf_path, max_pages=10):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(min(len(doc), max_pages)):\n",
    "        page = doc[page_num]\n",
    "        text.append(page.get_text())\n",
    "\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Step 6: Save SOP to Word (without images)\n",
    "def save_sop_to_word(sop_text: str, output_file: str):\n",
    "    doc = WordDocument()\n",
    "    doc.add_heading(\"Standard Operating Procedure (SOP)\", level=1)\n",
    "\n",
    "    # Split SOP text into sections\n",
    "    sections = sop_text.split(\"\\n\\n\")\n",
    "    for section in sections:\n",
    "        if \":\" in section:\n",
    "            header, content = section.split(\":\", 1)\n",
    "            doc.add_heading(header.strip(), level=2)\n",
    "            doc.add_paragraph(content.strip())\n",
    "        else:\n",
    "            doc.add_paragraph(section.strip())\n",
    "\n",
    "    doc.save(output_file)\n",
    "    print(f\"SOP successfully saved to {output_file}\")\n",
    "\n",
    "# Step 7: Generate the full SOP\n",
    "def generate_full_sop(query_text: str, manual_file: str, output_file: str):\n",
    "    sop_text = \"\"\n",
    "    sop_sections = [\n",
    "        \"Scope\",\n",
    "        \"Background\",\n",
    "        \"Safety\",\n",
    "        \"Materials Required\",\n",
    "        \"Standards and Controls\",\n",
    "        \"Calibration\",\n",
    "        \"Procedures\",\n",
    "        \"Sampling\",\n",
    "        \"Calculations\",\n",
    "        \"Uncertainty of Measurement\",\n",
    "        \"Limitations\",\n",
    "        \"Documentation\",\n",
    "        \"References\",\n",
    "    ]\n",
    "\n",
    "    for section in sop_sections:\n",
    "        print(f\"Generating section: {section}\")\n",
    "        section_text = generate_sop_section(query_text, manual_file, section)\n",
    "        sop_text += f\"\\n\\n{section}:\\n{section_text}\"\n",
    "\n",
    "    save_sop_to_word(sop_text, output_file)\n",
    "    print(\"SOP generation completed.\")\n",
    "\n",
    "# Step 8: Hide one SOP\n",
    "def hide_sop(sop_name: str, source_dir: str, hidden_dir: str):\n",
    "    sop_path = os.path.join(source_dir, sop_name)\n",
    "    hidden_path = os.path.join(hidden_dir, sop_name)\n",
    "\n",
    "    if not os.path.exists(hidden_dir):\n",
    "        os.makedirs(hidden_dir)\n",
    "\n",
    "    if os.path.exists(sop_path):\n",
    "        try:\n",
    "            shutil.copy(sop_path, hidden_path)\n",
    "            os.remove(sop_path)\n",
    "            print(f\"SOP '{sop_name}' has been hidden.\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"Permission error: {e}. Ensure the file is not open in another program.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while hiding SOP: {e}\")\n",
    "    else:\n",
    "        print(f\"SOP '{sop_name}' not found in '{source_dir}'.\")\n",
    "\n",
    "# Step 9: Extract and Compare File Content\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = WordDocument(docx_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def calculate_similarity(file1_path: str, file2_path: str):\n",
    "    if file1_path.endswith('.docx'):\n",
    "        file1_content = extract_text_from_docx(file1_path)\n",
    "    elif file1_path.endswith('.pdf'):\n",
    "        file1_content = extract_text_from_pdf(file1_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file1_path}\")\n",
    "\n",
    "    if file2_path.endswith('.docx'):\n",
    "        file2_content = extract_text_from_docx(file2_path)\n",
    "    elif file2_path.endswith('.pdf'):\n",
    "        file2_content = extract_text_from_pdf(file2_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file2_path}\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit_transform([file1_content, file2_content])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0, 1]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    hide_sop(HIDDEN_SOP, DATA_PATH, \"hidden_sops\")\n",
    "\n",
    "    print(\"Step 1: Loading documents...\")\n",
    "    documents = load_documents()\n",
    "\n",
    "    print(\"Step 2: Splitting documents into chunks...\")\n",
    "    chunks = split_documents(documents)\n",
    "\n",
    "    print(\"Step 3: Adding chunks to Chroma database...\")\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "    print(\"Step 4: Generating SOP for an instrument...\")\n",
    "    instrument_query = \"Write the SOP for ICPMS Liquids\"\n",
    "    generate_full_sop(instrument_query, MANUAL_FILE, OUTPUT_FILE)\n",
    "\n",
    "    hidden_sop_path = os.path.join(\"hidden_sops\", HIDDEN_SOP)\n",
    "    similarity_score = calculate_similarity(OUTPUT_FILE, hidden_sop_path)\n",
    "    print(f\"Similarity score between generated and actual SOP: {similarity_score:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcedd5-f9f6-4e12-b29f-91f33252333d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e31fe4-a41c-40d4-bda3-035f15412e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
